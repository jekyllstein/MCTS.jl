module MCTSExperiments

#repository from book authors https://github.com/algorithmsbooks/DecisionMakingProblems.jl which contains some of the basic functions and data structures and functions in the book as well as game environments
using DecisionMakingProblems
using Base.Threads

export DecisionMakingProblems, MDP, MonteCarloTreeSearch

#goal is to first use the basic functionality in the game2048 forked package and put it into a structure where the code from the algorithms book can be used with it

#the following code is copied from Appendix G.5 of the book found here: https://algorithmsbook.com/
function Base.findmax(f::Function, xs)
    f_max = -Inf
    x_max = first(xs)
    for x in xs
        v = f(x)
        if v > f_max
            f_max, x_max = v, x
        end
    end
    return f_max, x_max
end

Base.argmax(f::Function, xs) = findmax(f, xs)[2]

#the following code is copied from Chapter 7 of the book found here: https://algorithmsbook.com/
# struct MDP
#     Î³ # discount factor
#     ğ’® # state space
#     ğ’œ # action space
#     T # transition function
#     R # reward function
#     TR # sample transition and reward
# end
import DecisionMakingProblems.MDP #same struct as found in their module

function lookahead(ğ’«::MDP, U, s, a)
    ğ’®, T, R, Î³ = ğ’«.ğ’®, ğ’«.T, ğ’«.R, ğ’«.Î³
    return R(s,a) + Î³*sum(T(s,a,sâ€²)*U(sâ€²) for sâ€² in ğ’®)
end

function lookahead(ğ’«::MDP, U::Vector, s, a)
    ğ’®, T, R, Î³ = ğ’«.ğ’®, ğ’«.T, ğ’«.R, ğ’«.Î³
    return R(s,a) + Î³*sum(T(s,a,sâ€²)*U[i] for (i,sâ€²) in enumerate(ğ’®))
end

struct ValueFunctionPolicy
    ğ’« # problem
    U # utility function
end

function greedy(ğ’«::MDP, U, s)
    u, a = findmax(a->lookahead(ğ’«, U, s, a), ğ’«.ğ’œ)
    return (a=a, u=u)
end

#following code is copied from Chapter 9 of the book found here: https://algorithmsbook.com/

#forward search functions
struct RolloutLookahead
    ğ’« # problem
    Ï€ # rollout policy
    d # depth
end

randstep(ğ’«::MDP, s, a) = ğ’«.TR(s, a)

function rollout(ğ’«, s, Ï€, d)
    ret = 0.0
    for t in 1:d
        a = Ï€(s)
        s, r = randstep(ğ’«, s, a)
        ret += ğ’«.Î³^(t-1) * r
    end
    return ret
end

function (Ï€::RolloutLookahead)(s)
    U(s) = rollout(Ï€.ğ’«, s, Ï€.Ï€, Ï€.d)
    return greedy(Ï€.ğ’«, U, s).a
end

#= 
need to understand the convensions in this struct to see how to define all the components
what does ğ’« need to have?
ğ’«.ğ’œ, ğ’«.TR, ğ’«.Î³

what does N need to have?
N[(s,a)], so a dictionary of counts indexed by state/action pairs

what does Q need to have?
Q[(s,a)], so it is the same structure as N, clearly Q is not a function on state/action pairs but just a lookup

d and m should just be integers



=#

struct MonteCarloTreeSearch
    ğ’«::MDP # problem
    N::Dict # visit counts
    Q::Dict # action value estimates
    d::Integer # depth
    m::Integer # number of simulations
    c::AbstractFloat # exploration constant
    U::Function # value function estimate
end

struct MonteCarloTreeSearchTreePar 
    ğ’«::MDP # problem
    N::Vector{T} where T <: Dict  # visit counts
    Q::Vector{T}  where T <: Dict # action value estimates
    d::Integer # depth
    m::Integer # number of simulations
    c::AbstractFloat # exploration constant
    U::Function # value function estimate
    n::Integer # number of trees to have in parallel
end

function (Ï€::MonteCarloTreeSearch)(s)
    for k in 1:Ï€.m
        simulate!(Ï€, s)
    end
    return argmax(a->Ï€.Q[(s,a)], Ï€.ğ’«.ğ’œ)
end

function (Ï€::MonteCarloTreeSearchTreePar)(s)
    @threads for i in 1:Ï€.n
        for k in 1:Ï€.m
            simulate!(Ï€, s, i)
        end
    end
    return argmax(a->reduce((d1, d2) -> combine_dicts(+, d1, d2), Ï€.Q[(s,a)]), Ï€.ğ’«.ğ’œ)
end

function combine_dicts(op::Function, d1::T, d2::T) where T <: Dict
   dout = T() 
   klist = union(keys(d1), keys(d2))
   for k in klist 
        v1 = haskey(d1, k) ? d1[k] : 0
        v2 = haskey(d2, k) ? d2[k] : 0
        dout[k] = op(v1, v2)
   end
   return dout
end

function simulate!(Ï€::MonteCarloTreeSearch, s, d=Ï€.d)
    if d â‰¤ 0
        return Ï€.U(s)
    end
    ğ’«, N, Q, c = Ï€.ğ’«, Ï€.N, Ï€.Q, Ï€.c
    ğ’œ, TR, Î³ = ğ’«.ğ’œ, ğ’«.TR, ğ’«.Î³
    if !haskey(N, (s, first(ğ’œ)))
        for a in ğ’œ
            N[(s,a)] = 0
            Q[(s,a)] = 0.0
        end
        return Ï€.U(s)
    end
    a = explore(Ï€, s)
    sâ€², r = TR(s,a)
    q = r + Î³*simulate!(Ï€, sâ€², d-1)
    N[(s,a)] += 1
    Q[(s,a)] += (q-Q[(s,a)])/N[(s,a)]
    return q
end

function simulate!(Ï€::MonteCarloTreeSearchTreePar, s, i, d=Ï€.d)
    if d â‰¤ 0
        return Ï€.U(s)
    end
    ğ’«, N, Q, c = Ï€.ğ’«, Ï€.N[i], Ï€.Q[i], Ï€.c
    ğ’œ, TR, Î³ = ğ’«.ğ’œ, ğ’«.TR, ğ’«.Î³
    if !haskey(N, (s, first(ğ’œ)))
        for a in ğ’œ
            N[(s,a)] = 0
            Q[(s,a)] = 0.0
        end
        return Ï€.U(s)
    end
    a = explore(Ï€, s)
    sâ€², r = TR(s,a)
    q = r + Î³*simulate!(Ï€, sâ€², i, d-1)
    N[(s,a)] += 1
    Q[(s,a)] += (q-Q[(s,a)])/N[(s,a)]
    return q
end

bonus(Nsa, Ns) = Nsa == 0 ? Inf : sqrt(log(Ns)/Nsa)

function explore(Ï€::MonteCarloTreeSearch, s)
    ğ’œ, N, Q, c = Ï€.ğ’«.ğ’œ, Ï€.N, Ï€.Q, Ï€.c
    Ns = sum(N[(s,a)] for a in ğ’œ)
    return argmax(a->Q[(s,a)] + c*bonus(N[(s,a)], Ns), ğ’œ)
end

function explore(Ï€::MonteCarloTreeSearchTreePar, s)
    ğ’œ = Ï€.ğ’«.ğ’œ
    N = reduce((d1, d2) -> combine_dicts(+, d1, d2), Ï€.N)
    Q = reduce((d1, d2) -> combine_dicts(+, d1, d2), Ï€.Q)
    c = Ï€.c
    Ns = sum(N[(s,a)] for a in ğ’œ)
    return argmax(a->Q[(s,a)] + c*bonus(N[(s,a)], Ns), ğ’œ)
end
end # module
