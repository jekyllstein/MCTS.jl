module MCTS

#goal is to first use the basic functionality in the game2048 forked package and put it into a structure where the code from the algorithms book can be used with it

#the following code is copied from Appendix G.5 of the book found here: https://algorithmsbook.com/
function Base.findmax(f::Function, xs)
    f_max = -Inf
    x_max = first(xs)
    for x in xs
        v = f(x)
        if v > f_max
            f_max, x_max = v, x
        end
    end
    return f_max, x_max
end

Base.argmax(f::Function, xs) = findmax(f, xs)[2]

#the following code is copied from Chapter 7 of the book found here: https://algorithmsbook.com/
struct MDP
    Î³ # discount factor
    ğ’® # state space
    ğ’œ # action space
    T # transition function
    R # reward function
    TR # sample transition and reward
end

function lookahead(ğ’«::MDP, U, s, a)
    ğ’®, T, R, Î³ = ğ’«.ğ’®, ğ’«.T, ğ’«.R, ğ’«.Î³
    return R(s,a) + Î³*sum(T(s,a,sâ€²)*U(sâ€²) for sâ€² in ğ’®)
end

function lookahead(ğ’«::MDP, U::Vector, s, a)
    ğ’®, T, R, Î³ = ğ’«.ğ’®, ğ’«.T, ğ’«.R, ğ’«.Î³
    return R(s,a) + Î³*sum(T(s,a,sâ€²)*U[i] for (i,sâ€²) in enumerate(ğ’®))
end

struct ValueFunctionPolicy
    ğ’« # problem
    U # utility function
end

function greedy(ğ’«::MDP, U, s)
    u, a = findmax(a->lookahead(ğ’«, U, s, a), ğ’«.ğ’œ)
    return (a=a, u=u)
end

#following code is copied from Chapter 9 of the book found here: https://algorithmsbook.com/

#forward search functions
struct RolloutLookahead
    ğ’« # problem
    Ï€ # rollout policy
    d # depth
end

randstep(ğ’«::MDP, s, a) = ğ’«.TR(s, a)

function rollout(ğ’«, s, Ï€, d)
    ret = 0.0
    for t in 1:d
        a = Ï€(s)
        s, r = randstep(ğ’«, s, a)
        ret += ğ’«.Î³^(t-1) * r
    end
    return ret
end

function (Ï€::RolloutLookahead)(s)
    U(s) = rollout(Ï€.ğ’«, s, Ï€.Ï€, Ï€.d)
    return greedy(Ï€.ğ’«, U, s).a
end

#= 
need to understand the convensions in this struct to see how to define all the components
what does ğ’« need to have?
ğ’«.ğ’œ, ğ’«.TR, ğ’«.Î³

what does N need to have?
N[(s,a)], so a dictionary of counts indexed by state/action pairs

what does Q need to have?
Q[(s,a)], so it is the same structure as N, clearly Q is not a function on state/action pairs but just a lookup

d and m should just be integers



=#

struct MonteCarloTreeSearch
    ğ’« # problem
    N # visit counts
    Q # action value estimates
    d # depth
    m # number of simulations
    c # exploration constant
    U # value function estimate
end

function (Ï€::MonteCarloTreeSearch)(s)
    for k in 1:Ï€.m
        simulate!(Ï€, s)
    end
    return argmax(a->Ï€.Q[(s,a)], Ï€.ğ’«.ğ’œ)
end

function simulate!(Ï€::MonteCarloTreeSearch, s, d=Ï€.d)
    if d â‰¤ 0
        return Ï€.U(s)
    end
    ğ’«, N, Q, c = Ï€.ğ’«, Ï€.N, Ï€.Q, Ï€.c
    ğ’œ, TR, Î³ = ğ’«.ğ’œ, ğ’«.TR, ğ’«.Î³
    if !haskey(N, (s, first(ğ’œ)))
        for a in ğ’œ
            N[(s,a)] = 0
            Q[(s,a)] = 0.0
        end
        return Ï€.U(s)
    end
    a = explore(Ï€, s)
    sâ€², r = TR(s,a)
    q = r + Î³*simulate!(Ï€, sâ€², d-1)
    N[(s,a)] += 1
    Q[(s,a)] += (q-Q[(s,a)])/N[(s,a)]
    return q
end

bonus(Nsa, Ns) = Nsa == 0 ? Inf : sqrt(log(Ns)/Nsa)

function explore(Ï€::MonteCarloTreeSearch, s)
    ğ’œ, N, Q, c = Ï€.ğ’«.ğ’œ, Ï€.N, Ï€.Q, Ï€.c
    Ns = sum(N[(s,a)] for a in ğ’œ)
    return argmax(a->Q[(s,a)] + c*bonus(N[(s,a)], Ns), ğ’œ)
end

end # module
